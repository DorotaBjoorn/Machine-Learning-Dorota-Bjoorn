{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kokchun/Machine-learning-AI22/blob/main/Lecture_code/L4-Regularization.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; for interacting with the code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regularized linear models\n",
    "\n",
    "- Ridge regression\n",
    "- LASSO\n",
    "- ElasticNet\n",
    "---\n",
    "\n",
    "This is the lecture note for **regularized linear models**\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that this lecture note gives a brief introduction to regularized linear models. I encourage you to read further about regularized linear models. </p>\n",
    "\n",
    "Read more:\n",
    "\n",
    "- [Regularized linear models medium](https://medium.com/analytics-vidhya/regularized-linear-models-in-machine-learning-d2a01a26a46)\n",
    "- [Ridge regression wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "- [Tikhonov regularization wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "- [Lasso regression wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
    "- [Korsvalidering](https://sv.wikipedia.org/wiki/Korsvalidering)\n",
    "- [Cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [Scoring parameter sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [ISLRv2 pp 198-205](https://www.statlearning.com/)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 19), (66, 19), (134,), (66,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv(\"../data/Advertising.csv\", index_col=0)\n",
    "X, y = df.drop(\"Sales\", axis = 1), df[\"Sales\"]\n",
    "\n",
    "# in exercise 2 Polynomial regression you've found the elbow in degree 4, as the error increases after that\n",
    "# however to be safe and we assume that the model shouldn't have too many interactions between different features, I will choose 3\n",
    "# please try with 4 and see how your evaluation score differs\n",
    "\n",
    "# transform features\n",
    "model_polynomial = PolynomialFeatures(3, include_bias=False)\n",
    "polynomial_features = model_polynomial.fit_transform(X)\n",
    "\n",
    "# important to not forget \n",
    "X_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# from 3 features we've featured engineered 19 with degree 3 polynom (x1, x1^2, x1^3, x1x2x3, ....)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature standardization\n",
    "Remove sample mean and divide by sample standard deviation \n",
    "\n",
    "$X' = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "LASSO, Ridge and Elasticnet regression that we'll use later require that the data is scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X_train mean -0.00, std 1.00\n",
      "Scaled X_test mean -0.12, std 1.12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train) # calculates parametes sigma and mu based on X_train and transforms X_transform\n",
    "scaled_X_test = scaler.transform(X_test) # uses calculated mu and sigma to tranform X_test\n",
    "\n",
    "print(f\"Scaled X_train mean {scaled_X_train.mean():.2f}, std {scaled_X_train.std():.2f}\")\n",
    "print(f\"Scaled X_test mean {scaled_X_test.mean():.2f}, std {scaled_X_test.std():.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regularization techniques - thikonov regularization - L2-regularization)\n",
    "\n",
    "Problem with overfitting was discussed in previous lecture. When model is too complex, data noisy and dataset is too small the model picks up patterns in the noise. The output of a linear regression is the weighted sum: \n",
    "$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n$ , where the weights $\\theta_i$ represents the importance of the $ith$ feature. Want to constrain the weights associated with noise, through regularization. We do this by adding a regularization term to the cost function used in training the model. Note that the cost function for evaluation now will differ from the training.\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> most regularization model requires scaling of data </p>\n",
    "\n",
    "---\n",
    "### Ridge regression \n",
    "Also called Tikhonov regularization or $\\ell_2$ regularization.\n",
    "\n",
    "$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the ridge parameter or the penalty term, which reduces variance by increasing bias. Observe that the sum starts from 1, so the bias term $\\theta_0$ is not affected by $\\lambda$. Therefore by the larger the $\\lambda$ the more $\\theta_i, i = {1,2,\\ldots}$ causes higher error. As variance is decreasing and bias increasing, the model fits worse to the training datas noise and generalizes better.\n",
    "\n",
    "From the closed form OLS solution to ridge regression, we see that $\\lambda = 0$ gives us the normal equation for linear regression: \n",
    "\n",
    "$\\hat{\\vec{\\theta}} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3748516441217865, 0.2650465950553601, 0.5148267621786576)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def ridge_regression(X_test, penalty=0): #X_test dvs det vi vill predicta på, när funktioien används så är det med scaled_X_test\n",
    "    # alpha = 0 should give linear regression\n",
    "    # alhpa is Ridge same as lambda in theory, i.e. penalty term. sklearn has chosen alpha to generalize their API\n",
    "    model_ridge = Ridge(alpha=penalty) \n",
    "    model_ridge.fit(scaled_X_train, y_train)\n",
    "    y_pred = model_ridge.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y_pred = ridge_regression(scaled_X_test, penalty = 0) # since panalty = 0 this is polynomial regression\n",
    "\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3748516441217832, 0.26504659505536404, 0.5148267621786614)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with linear regression -> RMSE very similar!\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# polynomial regression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(scaled_X_train, y_train)\n",
    "y_pred = model_linear.predict(scaled_X_test)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3716840615001231, 0.26202411194681025, 0.5118829084339603)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with penalty (own guess of 0.01)\n",
    "# to hyperparameter tune need validation data\n",
    "\n",
    "y_pred = ridge_regression(scaled_X_test, penalty = 0.01)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lasso regression - L1\n",
    "Lasso - Least Absolute Shrinkage and Selection Operator or $\\ell_1$ regularization. Cost function for Lasso is: \n",
    "\n",
    "$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda\\sum_{i=1}^n |\\theta_i|$\n",
    "\n",
    "It sets least important features to zero, when $\\lambda$ sufficiently large. This is practically feature selection. \n",
    "\n",
    "Note that in Lasso regression it is important to have scaled your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5735346450114956, 0.6168472080645071, 0.7853962108799017)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model_lasso = Lasso(alpha = .1)\n",
    "model_lasso.fit(scaled_X_train, y_train)\n",
    "y_pred = model_lasso.predict(scaled_X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross-validation\n",
    "\n",
    "One strategy to choose the best hyperparameter alpha is to take the training part of the data and \n",
    "1. shuffle dataset randomly\n",
    "2. split into k groups\n",
    "3. for each group -> take one test, the rest training -> fit the model -> predict on test -> get evaluation metric\n",
    "4. take the mean of the evaluation metrics\n",
    "5. choose the parameters and train on the entire training dataset\n",
    "\n",
    "Repeat this process for each alpha, to see which yielded lowest RMSE. k-fold cross-validation: \n",
    "- good for smaller datasets\n",
    "- fair evaluation, as a mean of the evaluation metric for all k groups is calculated\n",
    "- expensive to compute as it requires k+1 times of training\n",
    "\n",
    "---\n",
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV # ridge regression with cross-validation\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "#print(SCORERS.keys())\n",
    "# negative because sklearn uses convention of higher return values are better\n",
    "# cv is k, k-fold\n",
    "model_ridgeCV = RidgeCV(alphas = [.0001, .001, .01, .1, .5, 1, 5, 10], scoring = \"neg_mean_squared_error\") # passing in a list of alphas\n",
    "model_ridgeCV.fit(scaled_X_train, y_train)\n",
    "print(model_ridgeCV.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4343075766531668, 0.31763359449891565, 0.5635899169599432)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best alpha is 0.1\n",
    "# it seems that linear regression outperformed ridge regression in this case\n",
    "# however it could depend on the distribution of the train|test data, so using alpha = 0.1 is more robust here\n",
    "y_pred = model_ridgeCV.predict(scaled_X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.84681185,  0.52142086,  0.71689997, -6.17948738,  3.75034058,\n",
       "       -1.36283352, -0.08571128,  0.08322815, -0.34893776,  2.16952446,\n",
       "       -0.47840838,  0.68527348,  0.63080799, -0.5950065 ,  0.61661989,\n",
       "       -0.31335495,  0.36499629,  0.03328145, -0.13652471])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridgeCV.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression - L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.004968802520343366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DorotaBjöörn-AI22GBG\\.virtualenvs\\Machine-Learning-Dorota-Bjoorn-ugpTJiOr\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.548e-01, tolerance: 3.571e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# it is trying 100 different alphas along regularization path epsilon\n",
    "model_lassoCV = LassoCV(n_alphas = 200, cv=5) # cv is k in k-fold, model chooses 200 alphas, could also pass in a list of alphas to be tested\n",
    "model_lassoCV.fit(scaled_X_train, y_train)\n",
    "print(f\"alpha = {model_lassoCV.alpha_}\") # penelty (landa in theory) found through 5-fold cross validation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.11468536,  0.42127203,  0.28896055, -4.63391705,  3.48972093,\n",
       "       -0.390611  ,  0.        ,  0.        ,  0.        ,  1.24969939,\n",
       "       -0.        ,  0.        ,  0.13795383, -0.01666923,  0.        ,\n",
       "        0.        ,  0.10974819,  0.        ,  0.0458376 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we notice that many coefficients have been set to 0 using Lasso\n",
    "# it has selected some features for us \n",
    "# 0. means that those features have been removed\n",
    "#5.196 is beta_0\n",
    "# 19th values since had 19 features after feature engineering\n",
    "model_lassoCV.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46802072322691207, 0.3410150044071009, 0.5839648999786724)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicing with lassoCV with calculated alpha\n",
    "y_pred = model_lassoCV.predict(scaled_X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net\n",
    "\n",
    "Elastic net is a combination of both Ridge l2-regularization and Lasso l1-regularization. The cost function to be minimized for elastic net is: \n",
    "\n",
    "$$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda\\left(\\alpha\\sum_{i=1}^n |\\theta_i| + \\frac{1-\\alpha}{2}\\sum_{i=1}^n \\theta_i^2\\right)$$\n",
    "\n",
    ", where $\\alpha$ here determines the ratio for $\\ell_1$ or $\\ell_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 ratio: 1.0\n",
      "alpha 0.004968802520343366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# note that alpha here is lambda in the theory\n",
    "# l1_ratio is alpha in the theory\n",
    "model_elastic = ElasticNetCV(l1_ratio=[.1, .5, .7, .8, .9, .95, .99, 1], eps = 0.001, n_alphas = 100, max_iter=int(1e4))\n",
    "model_elastic.fit(scaled_X_train, y_train)\n",
    "print(f\"L1 ratio: {model_elastic.l1_ratio_}\") # this would remove ridge and pick Lasso regression entirely\n",
    "print(f\"alpha {model_elastic.alpha_}\") # same alphs as when we used ridge\n",
    "\n",
    "# L1-ratio  = 1 so LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46291883026932984, 0.33467924600222104, 0.5785146895301977)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_elastic.predict(scaled_X_test)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE, MSE, RMSE\n",
    "# note that the result is same for Lasso regression which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-Learning-Dorota-Bjoorn-ugpTJiOr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74d6eb8ce498fd91a2ca86b0b65600ff300631eb42ae7429d586d9932a69bb8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
