{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from my_functions import remove_outliers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation/cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../lab/data/cardio_train.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age recalculated from days to years\n",
    "df['age'] = df['age']/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change ome col names\n",
    "df = df.rename(columns={'ap_hi': 'systolic_high', 'ap_lo': 'diastolic_low', 'gluc': 'glucose', 'alco': 'alcohol', 'cardio': 'cardio_disease'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values in columns containing cathegorical values\n",
    "df[\"gender\"].value_counts(), df[\"cholesterol\"].value_counts(), df[\n",
    "    \"glucose\"\n",
    "].value_counts(), df[\"smoke\"].value_counts(), df[\"alcohol\"].value_counts(), df[\n",
    "    \"active\"\n",
    "].value_counts(), df[\n",
    "    \"cardio_disease\"\n",
    "].value_counts()\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# all seem ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check reasonalbility of columns containing non-cathegorical values\n",
    "df.describe()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# age - reasonable\n",
    "# hight - very low and very high values present (most humans can be assumed to be between 145-200 cm)\n",
    "# weight - too low values present (consideing min age is 26 years, reasonable with margins 30 and above)\n",
    "# systolic_high - too low and too high values present (reasonable with margins between 50 - 250)\n",
    "# diastolic_low - too low and too high values present (reasonable with margins between 20 - 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing all data\n",
    "fig, axes = plt.subplots(2,2, figsize = (16,8))\n",
    "y_cols = ['height', 'weight', 'systolic_high', 'diastolic_low']\n",
    "\n",
    "# axes.faltten creates a list-like object ([axes[0][0], axes[0][1], axes[1][0], axes[1][1]], ....)\n",
    "for ax, y_col in zip(axes.flatten(), y_cols):\n",
    "    sns.scatterplot(data=df, x=df.index, y= y_col, ax = ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Height:\n",
    "* values below 100 are suspected to be typos where 100-s was missed. can not be sure, thus remove those datapoints\n",
    "* most humas should be between 145-200 cm thus remove datapoints which are not between 125-220 cm\n",
    "\n",
    "Weight:\n",
    "* values below 30 are removed\n",
    "\n",
    "Systolic_high:\n",
    "* keep values 50-250\n",
    "\n",
    "Diastolic_low\n",
    "* keep values 20-180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows with datapoints which are not physically feasable\n",
    "df = df.query(\n",
    "    \"height >= 125 & height <= 220 & weight >= 30 & systolic_high <= 250 & systolic_high >= 50 & diastolic_low <= 180 & diastolic_low >= 20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing cleaned data\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize = (16,8))\n",
    "y_cols = ['height', 'weight', 'systolic_high', 'diastolic_low']\n",
    "\n",
    "for ax, y_col in zip(axes.flatten(), y_cols):\n",
    "    sns.scatterplot(data=df, x=df.index, y= y_col, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "#-------------------------------------\n",
    "# while cleaning data 1298 points were removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) How many are positive and how many are negative for cardio-vascular disease?\n",
    "34699 negative, 34003 positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cardio_disease'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b). What are fractions of people with 1: normal, 2: above normal, 3: well above normal cholesterol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cholesterol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['cholesterol'].value_counts(), autopct='%.0f%%', labels=['normal cholesterol', 'above normal', 'well above normal']);\n",
    "plt.title('Fractions of people with 3 levels of cholesterol');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Plot histogram over age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x='age').set(title = 'Age distribution');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) What percentage smokes?\n",
    "8.8 % smoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * (df['smoke'].sum()/len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Plot weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='weight', bins=40).set(title = 'Weight distribution');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Plot height distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='height', bins=40).set(title = 'Height distribution');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) What percentage of women and men have cardiovascular desease?\n",
    "49 % of women, 50 % of men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_women = 100 * len(df.query('gender == 1 & cardio_disease == 1'))/len(df.query('gender == 1'))\n",
    "percentage_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_men = 100 * len(df.query('gender == 2 & cardio_disease == 1'))/len(df.query('gender == 2'))\n",
    "percentage_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_women = df[df['gender'] == 1]\n",
    "plt.pie(df_women['cardio_disease'].value_counts(), autopct='%.1f%%', labels=['healthy', 'with cardio-vascular disease']);\n",
    "plt.title('Women');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_men = df[df['gender'] == 2]\n",
    "plt.pie(df_men['cardio_disease'].value_counts(), autopct='%.1f%%', labels=['healthy', 'with cardio-vascular disease']);\n",
    "plt.title('Men');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.0 Feature engineering BMI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BMI feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'] = df['weight']/(0.0001 * df['height'] * df['height'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Clean BMI column from outliers and not reasonable values\n",
    "Min and max thresholds for BMI according to theory are < 16.0 and >= 40, thus BMI in our data far from these can probably be considered ouliers. For a more stringent approach Tukeys rule can be used, where outliers can be visulaized with boxplot. In our data removing ouliers accoring to Tukeys rule results in BMI_min = 14.5 and BMI_max = 39.5, which is in the same range as theoretical cut-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplott and corresponding boxplot of all BMI i dataset\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (8, 4))\n",
    "sns.scatterplot(data=df, x=df.index, y= 'BMI', ax=ax[0]);\n",
    "sns.boxplot(data=df, x=\"BMI\", ax=ax[1]);\n",
    "fig.suptitle('BMI in dataset prior to removal of ouliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df, 'BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'].min(), df['BMI'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (8, 4))\n",
    "sns.scatterplot(data=df, x=df.index, y= 'BMI', ax=ax[0]);\n",
    "sns.boxplot(data=df, x=\"BMI\", ax=ax[1]);\n",
    "fig.suptitle('BMI in dataset after removal of ouliers')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Create a categorical BMI feature\n",
    "|Category          |threshold      | index |\n",
    "|-------------------|---------------|-------|\n",
    "|Underweight (Thin) |\t< 18.5      |1      |\n",
    "|Normal range    \t|18.5 – 24.9    |2     |\n",
    "|Overweight     \t|25.0 – 29.9    |3     |\n",
    "|Obesity (Class 1)\t|30.0 – 34.9    |4      |\n",
    "|Obesity (Class 2)\t|35.0 – 39.9    |5    |\n",
    "| Obesity (Class 3)\t|≥ 40.0         |6      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_BMI(BMI):\n",
    "    if BMI < 18.5:\n",
    "        return 1\n",
    "    if BMI >= 18.5 and BMI < 25:\n",
    "        return 2\n",
    "    if BMI >= 25 and BMI < 30:\n",
    "        return 3\n",
    "    if BMI >= 30 and BMI < 35:\n",
    "        return 4\n",
    "    if BMI >= 35 and BMI < 40:\n",
    "        return 5\n",
    "    if BMI >= 40:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI_category'] = df['BMI'].apply(categorize_BMI)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1 Feature engineering blood pressure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systolic pressure - removing outliers\n",
    "Min and max thresholds for systolic blood pressure according to referenced article are < 120 mmHg and > 180 mmHg. After removing outliers using Tukeys rule systolic bloodpressure values in dataset were between 93 - 169 mmHg, which is reasonably close to min/max thresholds. Possibly some points at high end should not have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplott and corresponding boxplot of all systolic pressure i dataset\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (8, 4))\n",
    "sns.scatterplot(data=df, x=df.index, y= 'systolic_high', ax=ax[0]);\n",
    "sns.boxplot(data=df, x=\"systolic_high\", ax=ax[1]);\n",
    "fig.suptitle('Systolic pressure in dataset prior to removal of ouliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df, 'systolic_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['systolic_high'].min(), df['systolic_high'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplott and corresponding boxplot of all systolic pressure i dataset\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (8, 4))\n",
    "sns.scatterplot(data=df, x=df.index, y= 'systolic_high', ax=ax[0]);\n",
    "sns.boxplot(data=df, x=\"systolic_high\", ax=ax[1]);\n",
    "fig.suptitle('Systolic pressure in dataset after removal of ouliers')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diastolic pressure - removing outliers\n",
    "Min and max thresholds for diastolic blood pressure according to referenced article are < 80 mmHg and > 120 mmHg. After removing outliers using Tukeys rule diastolic bloodpressure values in dataset were between 66 - 104 mmHg, which is reasonably close to min/max thresholds. Possibly some points at high end should not have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplott and corresponding boxplot of all diastolic pressure i dataset\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (8, 4))\n",
    "sns.scatterplot(data=df, x=df.index, y= 'diastolic_low', ax=ax[0]);\n",
    "sns.boxplot(data=df, x=\"diastolic_low\", ax=ax[1]);\n",
    "fig.suptitle('Diastolic pressure in dataset prior to removal of ouliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df, 'diastolic_low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diastolic_low'].min(), df['diastolic_low'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplott and corresponding boxplot of all diastolic pressure i dataset\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (8, 4))\n",
    "sns.scatterplot(data=df, x=df.index, y= 'diastolic_low', ax=ax[0]);\n",
    "sns.boxplot(data=df, x=\"diastolic_low\", ax=ax[1]);\n",
    "fig.suptitle('Diastolic pressure in dataset after removal of ouliers')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataset after outlier removal contains 61910 samples out of the original 70000."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a categorical blood pressure feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Category               | index  |\n",
    "|-------------------    |------- |\n",
    "|Healthy                |1       |\n",
    "|Elevated          \t    |2       |\n",
    "|Stage 1 hypertension   |3       |\n",
    "|Stage 2 hypertension\t|4       |\n",
    "\n",
    "\n",
    "thresholds according to table https://www.healthline.com/health/high-blood-pressure-hypertension#symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_blood_pressure(systolic, diastolic):\n",
    "    \"\"\"Categorizes blood pressure 1-4 depending on systolic and diastolic pressures\"\"\"\n",
    "    if  systolic < 120 and diastolic < 80:\n",
    "        return 1\n",
    "    if  systolic >= 120 and systolic <130 and diastolic < 80:\n",
    "        return 2\n",
    "    if  systolic >= 130 and systolic <140 or diastolic >= 80 and diastolic < 90:\n",
    "        return 3\n",
    "    if  systolic >= 140 or diastolic >= 90:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = df.apply(lambda row: categorize_blood_pressure(row['systolic_high'], row['diastolic_low']), axis = 1)\n",
    "df.insert(loc = 6, column='blood_pressure_category', value=new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.0 Visualisation - percentage disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['BMI_category','blood_pressure_category', 'cholesterol','glucose', 'smoke', 'alcohol', 'active', 'gender']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize = (14,8), sharey=True)\n",
    "fig.text(0.07, 0.5,'fraction with cardio disease', va='center', rotation='vertical')\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "for ax, column in zip(axes.flatten(), columns):\n",
    "    sns.barplot(df, x= column, y='cardio_disease', ax=ax).set(ylabel = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Visualisation - correlation\n",
    "Strong correlation can be seen between BMI - BMI_category - weight, all of each are dependant on eachother <br>\n",
    "Strong correlation can also be seen between systolic - diastolic - blood pressure category which are dependant on eachother <br>\n",
    "There is a correlation between systolic and diastolic blood pressures <br>\n",
    "Onset of disease seems to be moslty correlated to systolic blood pressure followed by diastolic blood pressure, age, BMI. However all of which are weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Create 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.drop(columns=['systolic_high', 'diastolic_low', 'height', 'weight', 'BMI'])\n",
    "df_1 = pd.get_dummies(data=df_1, columns=['BMI_category', 'blood_pressure_category', 'gender'], drop_first=True)\n",
    "df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.drop(columns=['BMI_category', 'blood_pressure_category', 'height', 'weight'])\n",
    "df_2 = pd.get_dummies(data=df_2, columns=['gender'], drop_first= True)\n",
    "df_2.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Choose model\n",
    "Following models are evaluated for both datasets:\n",
    "* logisic regression\n",
    "* KNN classifier\n",
    "* SVM classifier\n",
    "* random forrest\n",
    "\n",
    "cv = 5 will be used for all model training, which is a common choice<br>\n",
    "scoring = 'recall' as metric since we want to minimize false negative responses as dataset is to be used for diagnostics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train|val|test split (df_1) - 60|20|20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61910, 14), (61910,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, y1 = df_1.drop(columns='cardio_disease'), df_1['cardio_disease']\n",
    "X1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37146, 14), (12382, 14), (12382, 14), (37146,), (12382,), (12382,))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_trainval, X1_test, y1_trainval, y1_test = train_test_split(X1, y1, test_size=0.20)\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1_trainval, y1_trainval, test_size=0.25)\n",
    "\n",
    "X1_train.shape, X1_val.shape, X1_test.shape, y1_train.shape, y1_val.shape, y1_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaling data (df_1)\n",
    "Choosing feature standardisation. MinMax scaler could have been evaluated too, but it is not done within the scope of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_X1_train = scaler.fit_transform(X1_train)\n",
    "scaled_X1_val = scaler.transform(X1_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression - df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.8, 'l1_ratio': 0}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameters: 'l1-ratio', 'C'\n",
    "# 1st round: param_grid = {\"l1_ratio\": np.linspace(0, 1, 20), \"C\": [0.1, 1, 10, 100]} -> 0 (thus Ridge indicated by l2), 1\n",
    "# 2nd round: param_grid = {\"l1_ratio\": [0, 0.1], \"C\": [0.5, 0.6, 0.7, 0.8, 0.9] - > 0, 0.8\n",
    "\n",
    "param_grid = {\"l1_ratio\": [0, 0.1], \"C\": [0.5, 0.6, 0.7, 0.8, 0.9]} \n",
    "model_1_lr = GridSearchCV(LogisticRegression(penalty = 'elasticnet', solver='saga', max_iter=10000), param_grid=param_grid, cv=5, scoring=\"recall\")\n",
    "model_1_lr.fit(scaled_X1_train, y1_train)\n",
    "model_1_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 69.35 %\n",
      "Score val: 69.19 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameters: l1-ratio = 0 (<=> penalty = l2), C = 0.8\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_1_lr = LogisticRegression(penalty = 'l2', solver='lbfgs', max_iter=10000, C=0.8)\n",
    "model_1_lr.fit(scaled_X1_train, y1_train)\n",
    "y1_pred_lr = model_1_lr.predict(scaled_X1_val)\n",
    "\n",
    "print(f'Score train: {100 * model_1_lr.score(scaled_X1_train, y1_train):.2f} %')\n",
    "print(f'Score val: {100 * model_1_lr.score(scaled_X1_val, y1_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN neighbors - df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 17}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameter: n_neighbors\n",
    "# 1st round: param_grid = {\"n_neighbors\": range(1,30)} -> 17\n",
    "\n",
    "param_grid = {\"n_neighbors\": range(1,30)} \n",
    "model_1_knn = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=5, scoring=\"recall\")\n",
    "model_1_knn.fit(scaled_X1_train, y1_train)\n",
    "model_1_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 71.52 %\n",
      "Score val: 67.64 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameter: n_neighbors = 17\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_1_knn = KNeighborsClassifier(n_neighbors=17)\n",
    "model_1_knn.fit(scaled_X1_train, y1_train)\n",
    "y1_pred_knn = model_1_knn.predict(scaled_X1_val)\n",
    "\n",
    "print(f'Score train: {100 * model_1_knn.score(scaled_X1_train, y1_train):.2f} %')\n",
    "print(f'Score val: {100 * model_1_knn.score(scaled_X1_val, y1_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM - df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameters: C, kernel\n",
    "# 1st round: param_grid = {\"C\": [0.1, 1, 10], \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid']} -> 10, rbf\n",
    "# could finetune C but since calculations take long time it will not be done in this lavb\n",
    "\n",
    "# param_grid = {\"C\": [0.1, 1, 10], \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid']} \n",
    "# model_1_svm = GridSearchCV(SVC(), param_grid=param_grid, cv=5, verbose=1, scoring=\"recall\")\n",
    "# model_1_svm.fit(scaled_X1_train, y1_train)\n",
    "# model_1_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 70.74 %\n",
      "Score val: 69.05 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameter: C = 10, kernel = rbf\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_1_svm = SVC(C=10, kernel=\"rbf\")\n",
    "model_1_svm.fit(scaled_X1_train, y1_train)\n",
    "y1_pred_svm = model_1_svm.predict(scaled_X1_val)\n",
    "\n",
    "print(f\"Score train: {100 * model_1_svm.score(scaled_X1_train, y1_train):.2f} %\")\n",
    "print(f\"Score val: {100 * model_1_svm.score(scaled_X1_val, y1_val):.2f} %\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forrest - df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'n_estimators': 300}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameters: n_estimators, max_depth\n",
    "# Fixed criterion = 'gini', max_features = sqrt\n",
    "# 1st round: param_grid = {\"n_estimators\": [100, 150, 200, 300], \"max_depth\": [5, 10, 15, 20]} -> 300, 10\n",
    "# 2nd tuning: param_grid = {\"n_estimators\": [300, 350], \"max_depth\": [10]} -> 300, 10\n",
    "\n",
    "param_grid = {\"n_estimators\": [300, 350], \"max_depth\": [10]} \n",
    "model_1_rf = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=5, verbose=1, scoring=\"recall\")\n",
    "model_1_rf.fit(X1_train, y1_train)\n",
    "model_1_rf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 71.48 %\n",
      "Score val: 69.21 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameters: 'max_depth': 10, 'n_estimators': 300 and calculate scores for model on train and val data\n",
    "model_1_rf = RandomForestClassifier(n_estimators=300, max_depth=10)\n",
    "model_1_rf.fit(X1_train, y1_train)\n",
    "y1_pred_rf = model_1_rf.predict(X1_val)\n",
    "\n",
    "print(f'Score train: {100 * model_1_rf.score(X1_train, y1_train):.2f} %')\n",
    "print(f'Score val: {100 * model_1_rf.score(X1_val, y1_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### train|val|test split (df_2) - 60|20|20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61910, 10), (61910,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_2\n",
    "X2, y2 = df_2.drop(columns='cardio_disease'), df_2['cardio_disease']\n",
    "X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37146, 10), (12382, 10), (12382, 10), (37146,), (12382,), (12382,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_trainval, X2_test, y2_trainval, y2_test = train_test_split(X2, y2, test_size=0.20)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_trainval, y2_trainval, test_size=0.25)\n",
    "\n",
    "X2_train.shape, X2_val.shape, X2_test.shape, y2_train.shape, y2_val.shape, y2_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaling data (df_2)\n",
    "Choosing feature standardisation. MinMax scaler could have been evaluated too, but it is not done within the scope of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_X2_train = scaler.fit_transform(X2_train)\n",
    "scaled_X2_val = scaler.transform(X2_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression - df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'l1_ratio': 0.4}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameters: 'l1-ratio', 'C'\n",
    "# 1st round: param_grid = {\"l1_ratio\": np.linspace(0, 1, 20), \"C\": [0.1, 1, 10, 100]} -> 0.42, 0.1\n",
    "# 2nd round: param_grid = {\"l1_ratio\": [0.3, 0.4, 0.5], \"C\": [0.01, 0.05, 0.1, 0.15] - > 0.4, 0.1\n",
    "\n",
    "param_grid = {\"l1_ratio\": [0.3, 0.4, 0.5], \"C\": [0.01, 0.05, 0.1, 0.15]}\n",
    "model_2_lr = GridSearchCV(LogisticRegression(penalty = 'elasticnet', solver='saga', max_iter=10000), param_grid=param_grid, cv=5, scoring=\"recall\")\n",
    "model_2_lr.fit(scaled_X2_train, y2_train)\n",
    "model_2_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 72.08 %\n",
      "Score val: 71.79 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameters: l1-ratio = 0.42, C = 0.1\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_2_lr = LogisticRegression(penalty = 'elasticnet', solver='saga', max_iter=10000, C=0.1, l1_ratio=0.42)\n",
    "model_2_lr.fit(scaled_X2_train, y2_train)\n",
    "y2_pred_lr = model_2_lr.predict(scaled_X2_val)\n",
    "\n",
    "print(f'Score train: {100 * model_2_lr.score(scaled_X2_train, y2_train):.2f} %')\n",
    "print(f'Score val: {100 * model_2_lr.score(scaled_X2_val, y2_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN neighbors - df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 15}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameter: n_neighbors\n",
    "# 1st round: param_grid = {\"n_neighbors\": range(1,30)} -> 15\n",
    "\n",
    "param_grid = {\"n_neighbors\": range(1,30)} \n",
    "model_2_knn = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=5, scoring=\"recall\")\n",
    "model_2_knn.fit(scaled_X2_train, y2_train)\n",
    "model_2_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 74.45 %\n",
      "Score val: 71.36 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameter: n_neighbors = 15\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_2_knn = KNeighborsClassifier(n_neighbors=15)\n",
    "model_2_knn.fit(scaled_X2_train, y2_train)\n",
    "y2_pred_knn = model_2_knn.predict(scaled_X2_val)\n",
    "\n",
    "print(f'Score train: {100 * model_2_knn.score(scaled_X2_train, y2_train):.2f} %')\n",
    "print(f'Score val: {100 * model_2_knn.score(scaled_X2_val, y2_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM - df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameters: C, kernel\n",
    "# 1st round: param_grid = {\"C\": [0.1, 1, 10], \"kernel\": ['linear', 'rbf']} -> 0.1, rbf\n",
    "\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"kernel\": ['linear','rbf']} \n",
    "model_2_svm = GridSearchCV(SVC(), param_grid=param_grid, cv=5, verbose=1, scoring=\"recall\")\n",
    "model_2_svm.fit(scaled_X2_train, y2_train)\n",
    "model_2_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 72.66 %\n",
      "Score val: 72.44 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameter: C=.1, kernel = rbf\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_2_svm = SVC(C=0.1, kernel='rbf')\n",
    "model_2_svm.fit(scaled_X2_train, y2_train)\n",
    "y2_pred_svm = model_2_svm.predict(scaled_X2_val)\n",
    "\n",
    "print(f'Score train: {100 * model_2_svm.score(scaled_X2_train, y2_train):.2f} %')\n",
    "print(f'Score val: {100 * model_2_svm.score(scaled_X2_val, y2_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forrest - df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15, 'n_estimators': 100}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter tuning of hyperparameters: n_estimators, max_depth\n",
    "# Fixed criterion = 'gini', max_features = sqrt\n",
    "# 1st round: param_grid = {\"n_estimators\": [100, 200, 300, 400], \"max_depth\": [5, 10, 15, 20]} -> 300, 15\n",
    "# 2nd tuning: param_grid = {\"n_estimators\": [10, 100, 200, 300], \"max_depth\": [15]} -> 100, 15\n",
    "# 3rd tuning: param_grid = {\"n_estimators\": [10, 50, 100, 150], \"max_depth\": [15]}  -> 100, 15\n",
    "\n",
    "param_grid = {\"n_estimators\": [10, 50, 100, 150], \"max_depth\": [15]} \n",
    "model_2_rf = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=5, verbose=1, scoring=\"recall\")\n",
    "model_2_rf.fit(X2_train, y2_train)\n",
    "model_2_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 82.20 %\n",
      "Score val: 72.50 %\n"
     ]
    }
   ],
   "source": [
    "# train model with chosen parameter: n_estimators=100, max_depth=15\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_2_rf = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "model_2_rf.fit(X2_train, y2_train)\n",
    "y2_pred_rf = model_2_rf.predict(X2_val)\n",
    "\n",
    "print(f'Score train: {100 * model_2_rf.score(X2_train, y2_train):.2f} %')\n",
    "print(f'Score val: {100 * model_2_rf.score(X2_val, y2_val):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of scores from all models:\n",
    "|model          | dataset       |score - train  |score - val    |\n",
    "|---------------|---------------|---------------|---------------|\n",
    "|logistic reg   |df_1           |69.35 %        |69.19 %        |\n",
    "|KNN            |df_1           |71.52 %        |67.64 %        |\n",
    "|SVM            |df_1           |70.74 %        |69.05 %        |\n",
    "|random forrest |df_1           |71.48 %        |69.12 %        |\n",
    "|logistic reg   |df_2           |72.08 %        |71.79 %        |\n",
    "|KNN            |df_2           |74.45 %        |71.36 %        |\n",
    "|SVM            |df_2           |72.66 %        |72.44 %        |\n",
    "|random forrest |df_2           |82.20 %        |72.50 %        |\n",
    "\n",
    "Conlusions:\n",
    "* all tested models result in similar (relatively poor) scores\n",
    "* score for train data and val data are similar in all cases indicating models are not overfitted (random forrest df2 possibly exception)\n",
    "* df_2 results generally in somewhat higher scores indicating that having acual values for featuras rather than categorizing them is advantegous\n",
    "* SVM least prefered choice since time consuming and not better than others\n",
    "* logistic regression by far fastest and relatively good result\n",
    "* KNN and random forrest similar in calculation time\n",
    "\n",
    "Choice: df_2 and ranom forrest since reasonable calculation time and somewhat better result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Training random forrest on df_2 train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score trainval: 80.96 %\n",
      "Score test: 72.85 %\n"
     ]
    }
   ],
   "source": [
    "# scaling not needed\n",
    "\n",
    "# training on train and val data\n",
    "model_trainval_rf = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "model_trainval_rf.fit(X2_trainval, y2_trainval)\n",
    "y2_trainval_pred_rf = model_trainval_rf.predict(X2_test)\n",
    "\n",
    "# printing scores for trainval och test data\n",
    "print(f'Score trainval: {100 * model_trainval_rf.score(X2_trainval, y2_trainval):.2f} %')\n",
    "print(f'Score test: {100 * model_trainval_rf.score(X2_test, y2_test):.2f} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling of data\n",
    "scaler = StandardScaler()\n",
    "scaled_X2_trainval = scaler.fit_transform(X2_trainval)\n",
    "scaled_X2_test = scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score trainval: 76.53 %\n",
      "Score test: 72.82 %\n"
     ]
    }
   ],
   "source": [
    "# listing modles to be included in voting (svm omitted to save calculation times)\n",
    "classifier_lg = LogisticRegression(penalty = 'elasticnet', solver='saga', max_iter=10000, C=0.1, l1_ratio=0.42)\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors=15)\n",
    "classifier_rf = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "\n",
    "# creating voting model with hard voting, training it and makeing a prediction\n",
    "model_vc = VotingClassifier(estimators=[('lg', classifier_lg), ('knn', classifier_knn), ('rf', classifier_rf)], voting = 'hard')\n",
    "model_vc = model_vc.fit(scaled_X2_trainval, y2_trainval)\n",
    "y2_trainval_pred_vc = model_vc.predict(scaled_X2_test)\n",
    "\n",
    "#printing scores for trainval och test data\n",
    "print(f'Score trainval: {100 * model_vc.score(scaled_X2_trainval, y2_trainval):.2f} %')\n",
    "print(f'Score test: {100 * model_vc.score(scaled_X2_test, y2_test):.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-Learning-Dorota-Bjoorn-ugpTJiOr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
